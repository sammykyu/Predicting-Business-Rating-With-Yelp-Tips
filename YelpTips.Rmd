---
title: "Business Rating Prediction Based on Yelp Tips"
author: "Sammy Yu"
date: "November 2, 2015"
output: html_document
---

### Introduction

The tips on the Yelp web site is the insider information provided by the Yelp's users or reviewers. The text of the tips contain the sentiments of the users of how they would recomend for the other users when they use the service or buy a product from a particular business. They can positive or negative and should be consistent with review posted by the same user. In this study, I will try to predict a business overall rating based on only the text of tips to see how the tips can affect the rating or image of the business.

### Methods and Data

To create and train a prediction model for business ratings, I use two datasets, "yelp_academic_dataset_business.json" and "yelp_academic_dataset_tips.json" from [Yelp Dataset Challenge](http://www.yelp.com/dataset_challenge) (Rounds 5 and 6).

Both files are in JSON format. The files will be read, merged and cleaned up before they are used for developing the prediction model.  


The algorithm I use to develop the prediction model is called the LASSO (Least Absolute Shrinkage and Selection Operator) which was developed by Robert Tibshirani in 1996. The LASSO is derived from multiple linear regression and is similar to Ridge Regression which penalize the coefficients to minimize overfitting. The LASSO has an advantage over Ridge Regression though is that it not only shrinks the coefficients but also force some of the coefficients to be exactly zero when the tuning parameter \lambda is selected large enough.  

$rating = \beta_{0} + \beta_{1}T_{1} + \beta_{1}T_{2} + ... + \beta_{p}T_{p} + \varepsilon$  

Where the $T_{p}$ is a popular word or term appears in the tips text, such as "great".  
The goal of the LASSO is to find the coefficients those can minimize the quantity below:  

$RSS + \lambda\sum_{j=1}^{p}{|}\beta_{j}{|}$  

Where $RSS(Root Square Errors) = \sum_{i=1}^{n}({rating}_{i} - \beta_{0} - \sum_{j=1}^{p}\beta_{j}{T}_{ij})^2$    



```{r, echo=FALSE, eval=FALSE}
memory.size(max=30000)
library(jsonlite)
library(NLP)
library(tm)
library(SnowballC)
library(wordcloud)
library(glmnet)

```

#### Reading data
```{r, cache=TRUE, eval=FALSE, echo=FALSE}
bus <- stream_in(file("yelp_academic_dataset_business.json"))
tips <- stream_in(file("yelp_academic_dataset_tip.json"))

```

#### User defined functions
```{r, eval=FALSE}

PlotWordCloud <- function (docTermMatrix) {
  ## Find frequent terms
  frequency <- colSums(docTermMatrix)
  frequency <- sort(frequency, decreasing=TRUE)
  ## make word cloud
  words <- names(frequency)
  wordcloud(words, frequency, max.words=100)
}

GetModelingData <- function (docTermMatrix) {
  ## add the business ID column to the term matrix
  dtm_tips <- cbind(business_id=tips$business_id, as.data.frame(docTermMatrix))
  
  ## merge the business ratings with tip terms dataframe
  bustips <- merge(bus[,c("business_id","stars")], dtm_tips, by="business_id")
  ## business ID column is no longer needed
  bustips$business_id <- NULL

  ## prepare the training and test data
  x <- model.matrix(stars~., bustips)[,-1]
  y <- bustips$stars
  ## create a list of lambdas for regularization
  lambdas <- 10^seq(10,-2, length=100)
  set.seed(1)
  ## randomly get 70% of the data for training, 30% for validation
  train<-sample (1: nrow(x), nrow(x) * 0.7)

  return(list(x=x, y=y, train=train, lambdas=lambdas))
}

LassoRegression <- function (params) {
  train <- params$train
  test <- (-train)
  lasso.mod <- glmnet(params$x[train,], params$y[train], alpha=1, lambda=params$lambdas)
  set.seed(2)
  cv.out <- cv.glmnet(params$x[train,], params$y[train], alpha=1)
  bestlamda <- cv.out$lambda.min
  lasso.pred  <- predict(lasso.mod, s=bestlamda, newx=params$x[test,])
  mse <- mean((lasso.pred - params$y[test])^2)
  out <- glmnet (params$x, params$y, alpha=1, lambda=params$lambdas)
  lasso.pred.coef <- predict(out, type="coefficients", s=bestlamda)
  ## get the coefficients vector
  lasso.coef <- lasso.pred.coef[1:dim(lasso.pred.coef)[1],]
  
  return(list(mod=lasso.mod, bestlamda=bestlamda, mse=mse, coef=lasso.coef))
}

```

#### Text mining on the tip texts
```{r, cache=TRUE, eval=FALSE}

## set the tip texts as the source
vsource <- VectorSource(tips$text)
tiptexts <- Corpus(vsource)

## cleaning
tiptexts <- tm_map(tiptexts, content_transformer(tolower))
tiptexts <- tm_map(tiptexts, removeNumbers)
tiptexts <- tm_map(tiptexts, removePunctuation)
tiptexts <- tm_map(tiptexts, stripWhitespace)

## unigram data
tiptexts_ugm <- tm_map(tiptexts, removeWords, stopwords("english"))
tiptexts_ugm <- tm_map(tiptexts_ugm, stemDocument)
## create a document-term matrix from the tip texts
dtm_ugm <- DocumentTermMatrix(tiptexts_ugm)
## remove the terms with more than 99.6% sparcity from the dtm
dtm_ugm <- removeSparseTerms(dtm_ugm, 0.996)
## save the dtm as a regular matrix for later processing
dtm2_ugm <- as.matrix(dtm_ugm)

## bigrams data (we keep the stop words)
tiptexts_bgm <- tm_map(tiptexts, stemDocument)
## create a document-term matrix from the tip texts
dtm_bgm <- DocumentTermMatrix(tiptexts_bgm)
## remove the terms with more than 99.8% sparcity from the dtm
dtm_bgm <- removeSparseTerms(dtm_bgm, 0.998)
## save the dtm as a regular matrix for later processing
dtm2_bgm <- as.matrix(dtm_bgm)

```


#### Most frequent used terms in tips 
```{r, eval=FALSE}
PlotWordCloud(dtm2_ugm)

```

#### Naive model
```{r, eval=FALSE}
## The predicted rating is simply the average of the ratings in the training data
naive.pred <- mean(bus$stars)
## Mean square errors (MSE) of the naive model
naive.mse <- mean((naive.pred - bus$stars)^2)

```

#### Lasso regression using Unigrams
```{r, cache=TRUE, eval=FALSE}
data_ugm <- GetModelingData(dtm2_ugm)
result_ugm <- LassoRegression(data_ugm)

```

#### Lasso regression using Bigrams
```{r, cache=TRUE, eval=FALSE}
data_Bgm <- GetModelingData(dtm2_Bgm)
result_Bgm <- LassoRegression(data_Bgm)
```


### Results
```{r, eval=FALSE}
result_ugm$mse
result_ugm$coef[result_ugm$coef>0]
length(result_ugm$coef[result_ugm$coef>0])

```



### Discussion

